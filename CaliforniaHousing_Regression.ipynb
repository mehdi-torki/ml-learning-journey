{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEMOQxtxqIlsCrs8ywSsIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-torki/ml-learning-journey/blob/main/CaliforniaHousing_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#data = pd.read_csv(\"/content/drive/MyDrive/googleplaystore.csv\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def cost_function(X, w, b, y):\n",
        "    predictors = np.dot(X, w) + b\n",
        "    errors = predictors - y\n",
        "    return np.sum(np.square(errors)) / (2 * len(y))\n",
        "\n",
        "def gradient_descent(X, w, b, y, learning_rate):\n",
        "    predictors = np.dot(X, w) + b\n",
        "    errors = predictors - y\n",
        "\n",
        "    dw = np.dot(X.T, errors) / len(y)\n",
        "    db = np.sum(errors) / len(y)\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "    return w, b\n",
        "\n",
        "\n",
        "# Fetch dataset\n",
        "california_housing = fetch_california_housing()\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = california_housing.data    # NumPy array\n",
        "y = california_housing.target  # NumPy array\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0.0\n",
        "count = 0\n",
        "epsilon = 1e-6\n",
        "\n",
        "learning_rate = 0.001\n",
        "scalar = StandardScaler()\n",
        "X = scalar.fit_transform(X)\n",
        "\n",
        "prev_cost = float('inf')\n",
        "cost_history = []\n",
        "while count < 20000:\n",
        "    w, b = gradient_descent(X, w, b, y, learning_rate)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    cost = cost_function(X, w, b, y)\n",
        "    cost_history.append(cost)\n",
        "    if (count % 100 == 0):\n",
        "        print(f'cost {count}: {cost}')\n",
        "    if (abs(prev_cost - cost) < epsilon):\n",
        "        break\n",
        "\n",
        "    prev_cost = cost\n",
        "\n",
        "print(f'cost {count}: {cost_function(X, w, b, y)}')\n",
        "print(f\"w: {w[:5]}... (showing first 5 coefficients)\")\n",
        "print(f\"b: {b}\")\n",
        "\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost Function Convergence')\n",
        "plt.show()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oZRRpFc13s5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mJMYH7x8uVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What did you find most challenging in implementing gradient descent?\n",
        "\n",
        "The most challenging part of the implementation for me was that I had to think differently in order to perform computations in a vectorized way. My usual and routine way of thinking and programming didn’t naturally lead to using vectorized or matrix-based operations — which means I need to learn to approach these kinds of problems with a different mindset.\n",
        "\n",
        "2. What surprised you when comparing scaled vs. unscaled training?\n",
        "\n",
        "n the scaled training, convergence happened at a much faster rate, and that was really fascinating for me.\n",
        "\n",
        "3. What’s one real-world application you can imagine using this technique for?\n",
        "\n",
        "One of the most interesting applications in the real world that I find is to guess and predict future weather conditions based on a series of environmental and climatic variables."
      ],
      "metadata": {
        "id": "ZkYmupKyrsPT"
      }
    }
  ]
}